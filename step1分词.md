# 分词
---
&emsp;&emsp;分词可以说是让模型认识并学习人类语言的第一步，就像人类先认识汉字、然后在字的基础上又学会了词，然后在成长过程中不断学习和扩充新词，并使用词来说或写成句、成段。

&emsp;&emsp;类比到模型，如何分词，让模型“认识”到更多的词就是最基础也是最关键的一步，模型可以根据学习到的词含义，经过一系列的转化，输出有意义内容，如以前的BERT和现在的LLM等。
## 1.问题
这里记录一下开始学习分词时候思考的一些问题。
* 中文英文分词方法一定是不相同的，两种语言应该怎么去分词？
* 词汇表是预加载的，如果出现了不在词汇表中的词怎么办？（OOV out-of vocabulary）
* 分词粒度是什么样的，很明显分词的粒度不一样就会导致词汇表大小不一样，不同的分词方法也会导致训练的模型性能不一致。
* 不论中文还是英文，不同的表述方式中词的含义不一样，如何分词才能精准且不会有歧义。

## 2.分词方法
&emsp;&emsp;分词（tokenizer）本身作为一个研究方向，其中的内容展开太多，这里只基于问题对分词方法做简单总结。最终选择分词方法还是以能平衡缓解上述问题为标准，或者说可以选用现在最流行的方法玩一玩。

分词方法大致有两种：
* 按照不同算法思路的分词方法
* 按照不同粒度的分词方法


***A.字典匹配（字符串匹配）***
**核心思路：** 给一个充分大的词典，再给一段文本，按照一定的策略将文本和词典匹配，若某字符串在字典里则分一个词。
* 容易出现OOV问题
* 容易出现歧义问题

***B.基于统计模型***
**核心思路：** 大语料统计词频，比如n-gram，hmm方法。
* 复杂度高
* 上下文信息有限
* 数据稀疏

***C.基于深度学习模型***
1. 词粒度
词粒度英文可以直接按照空格和标点划分，中文不行
    * 词粒度能保留更多的语义信息
    * 序列划分后词元数量更短，处理起来更高效
    * 词汇是不断扩张的，对于新产生的不在词典里的词，会带来**OOV问题**
    * 按词为粒度进行划分，会导致词典庞大
    * 稀疏词、低频词无法在训练时有充分的语料进行训练
    * 难以学习到同一个词的不同形态，比如英文中的look和looks，按词划分的话需要对这两个分别进行训练

2. 字符粒度
英文中就是字母，中文就是字
    * 词表大大减少，而且**很大程度上解决了OOV问题**
    * 单个字符的语义信息过少
    * 模型输入序列会变得很长

3. 字词粒度
平衡上述两种的优缺点。方法有BPE、BBPE、ULM、wordpiece等。

* BPE:一种贪心算法，全称为字节对编码，适用于欧美拉丁文体系，本质是一种数据压缩方法，来自于一篇发表于1994年的论文：“A new algorithm for data compression”。它在数据压缩中原始的核心思路非常简单：每一步都将最常见的一对_相邻数据单位_替换为该数据中没有出现过的一个_新单位_，反复迭代直到满足停止条件 。比如有一段数据是“abbabyabya”，其中相邻字母对的组合中"ab"出现3次是出现最多，即可以使用一个字母Z去代替"ab"，数据形式压缩成："ZbZyZya"。以此类推，下一个'Zy'继续被替换成Y，数据形式变成："ZbYYa"。
* BBPE：和BPE算法思想基本相同，但是适用于中日文等。（BPE中英文就26个有限字母，而中文汉字是有生僻字的，会占用不必要的词表，且容易oov）其是从utf8字节层面计算的，只是相对于BPE更低一层。

## 3. 分词